<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="../css/style.css" />
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <title>Mehdi Hellou</title>
    </head>

    <body>
        <div id ="bloc_page">

            <header>
                <div id="main_title">
                    <h1>Mehdi Hellou</h1>
                    <h2>Robotics/AI student</h2>    
                </div>                

                <nav>
                    <ul>
                        <li> <a href="main.html" >Home</a> </li>
                        <li> <a href="projects.html" >Projects</a> </li>
                        <li><a href="contact.html" >Contact</a></li>
                    </ul>
                </nav>
            </header>


            <section>
                <h2 id="main_title">Self-improving agent based on reinforcement learning</h2>

                <div id="main_text">
                    <p>During my second Master in AI at Sorbonne University, I have worked into a school project which was to read a scientific article related to AI methods used in robotics, and reproduced these methods and compared the results from the paper. The <a href="#ling1992" aria-describedby="footnote-label">paper</a> focus in the using of Reinforcement Learning methods as the adaptive heuristic critic (AHC) learning <a href="#sutton1984" aria-describedby="footnote-label">architecture</a> <a href="#sutton1990" aria-describedby="footnote-label"></a>, and <a href="#watkins1989" aria-describedby="footnote-label">Q-learning</a> into a complex and simulated environment. The environment included an agent, enemies, food and obstacles. The purpose of the agent was to survive into the environment by using its sensors to avoid the enemies and obstacles, and collect food.</p> 

                    <p>During this project, we had to create the environment and the protagonists (agent and enemies), the sensors of the agent by using <a href="#rumelhart1986" aria-describedby="footnote-label">coarse coding</a> and the python GUI toolkit <strong>Tkinter</strong>. We also adjusted the movement of the characters to be compatible with the GUI Tkinter. Thereafter, we had to set up the learning part by implementing the Q-Learning method in using a neural network which allowed the agent to choose its movement autonomously. The neural network had three layers including an input layer (145 neurons) whose the values were given by the agent's sensors, a hidden layer (30 neurons) and an output layer (1 neuron) corresponding to the Q-Value. At the end of implementing, we had to compare our results illustrated by the learning curves, to the ones from the paper.</p>   

                    <figure>
                        <img src="../../images/iar_results_project.jpg" alt="Learning curves" />
                        <figcaption>Figure  - Learning curves showing the number of resources eaten by the agent during 300 plays with and without using experience replay (blue and green curves, respectively).</figcaption> 
                    </figure> 
                    
                    <p> The learning curves above depict the agent's performance over 300 training simulations. As expected, the agent gradually learns to collect resources and avoid being caught by the enemies. A short demo is provided below to consolidate the agent's performance and to understand how we built the simulation.</p>
                    <div class="demo">
                        <video src="../../video/project_iar_final_version.mp4" controls></video>
                    </div>
                </div>

                <ol>
                    <li id="ling1992">Long-Ji Lin. 1992. Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching. Mach. Learn. 8, 3–4 (May 1992), 293–321.</li>

                    <li id="sutton1984">Richard Stuart Sutton.Temporal Credit Assignment in Reinforcement Learning. PhD thesis, 1984. AAI8410337.</li>

                    <li id="sutton1990">Sutton, Richard & Barto, Andrew. (1990). Time-Derivative Models of Pavlovian Reinforcement.</li>

                    <li id="watkins1989"> Watkins, C. J. C. H.. "Learning from Delayed Rewards." PhD diss., King's College, Oxford, 1989.</li>

                    <li id="rumelhart1986">D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation. Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations. MIT Press, Cambridge, MA, USA, 318–362.</li>
                </ol>
            </section>

            <footer>
                <div id="logo" >
                    <a href ='https://www.linkedin.com/in/mehdi-hellou-92b677129' target="_blank" id="linkedin"><i class="fa fa-linkedin-square"></i></a>
                    <a href ='https://github.com/Mehdi-Hellou' target="_blank" id="github"><i class="fa fa-github"></i></a>
                    <p> Email me: helloumehdi@outlook.fr </p>
                </div>


                <p class="copyright">Copyright Mehdi Hellou - All rights reserved></p>
            </footer>

        </div>
    </body>
</html>